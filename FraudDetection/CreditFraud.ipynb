{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[This](https://www.kaggle.com/mlg-ulb/creditcardfraud) dataset contains a PCA transformed\n",
    "record of credit card transactions where 0.172% are fraudulent. The overall goal is to detect the fraudulent transactions. \n",
    "\n",
    "Because the fraudulent transaction make up a tiny portion of the data, the data set is highly imbalanced.  One must be careful using the usual classification metrics on such data sets. The purpose of this notebook is to explore custom training metrics in LightGBM and Keras that are well suited to imbalanced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/craigkleski/Library/Python/3.6/lib/python/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "sns.set_palette(\"Set2\")\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.utils import np_utils\n",
    "\n",
    "np.random.seed(149)\n",
    "\n",
    "data = pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes in classification problems I like to view the variance inflation factor (VIF) to check for multicollinearity. Though high VIF is [not always a problem](https://stats.stackexchange.com/questions/168622/why-is-multicollinearity-not-checked-in-modern-statistics-machine-learning) in machine learning, it can be useful to know in case our model needs to be tweaked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIF = pd.DataFrame()\n",
    "VIF[\"VIF\"] = [variance_inflation_factor(data.values, i) for i in range(data.shape[1])]\n",
    "VIF[\"features\"] = data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VIF</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.339858</td>\n",
       "      <td>Time</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.638237</td>\n",
       "      <td>V1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.900804</td>\n",
       "      <td>V2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.321018</td>\n",
       "      <td>V3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.172479</td>\n",
       "      <td>V4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.764441</td>\n",
       "      <td>V5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.528629</td>\n",
       "      <td>V6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.603517</td>\n",
       "      <td>V7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.098591</td>\n",
       "      <td>V8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.037715</td>\n",
       "      <td>V9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.208870</td>\n",
       "      <td>V10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.080378</td>\n",
       "      <td>V11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1.154440</td>\n",
       "      <td>V12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1.003473</td>\n",
       "      <td>V13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1.219562</td>\n",
       "      <td>V14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.014148</td>\n",
       "      <td>V15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1.080994</td>\n",
       "      <td>V16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1.227258</td>\n",
       "      <td>V17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1.033900</td>\n",
       "      <td>V18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1.040800</td>\n",
       "      <td>V19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2.233935</td>\n",
       "      <td>V20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.103107</td>\n",
       "      <td>V21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1.082408</td>\n",
       "      <td>V22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1.149293</td>\n",
       "      <td>V23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1.000773</td>\n",
       "      <td>V24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1.013526</td>\n",
       "      <td>V25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1.000539</td>\n",
       "      <td>V26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1.009509</td>\n",
       "      <td>V27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.001595</td>\n",
       "      <td>V28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>11.507919</td>\n",
       "      <td>Amount</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>2.093512</td>\n",
       "      <td>Class</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          VIF features\n",
       "0    2.339858     Time\n",
       "1    1.638237       V1\n",
       "2    3.900804       V2\n",
       "3    1.321018       V3\n",
       "4    1.172479       V4\n",
       "5    2.764441       V5\n",
       "6    1.528629       V6\n",
       "7    2.603517       V7\n",
       "8    1.098591       V8\n",
       "9    1.037715       V9\n",
       "10   1.208870      V10\n",
       "11   1.080378      V11\n",
       "12   1.154440      V12\n",
       "13   1.003473      V13\n",
       "14   1.219562      V14\n",
       "15   1.014148      V15\n",
       "16   1.080994      V16\n",
       "17   1.227258      V17\n",
       "18   1.033900      V18\n",
       "19   1.040800      V19\n",
       "20   2.233935      V20\n",
       "21   1.103107      V21\n",
       "22   1.082408      V22\n",
       "23   1.149293      V23\n",
       "24   1.000773      V24\n",
       "25   1.013526      V25\n",
       "26   1.000539      V26\n",
       "27   1.009509      V27\n",
       "28   1.001595      V28\n",
       "29  11.507919   Amount\n",
       "30   2.093512    Class"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VIF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, `Amount` has high VIF, suggesting multicollinearity. At the moment, we will ignore this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will try two different approaches to detecting fraud. In the first method, we will use LightGBM's gradient boosting classifier on the _full_ data set, carefully managing the imbalance with scaling and a custom metric.  Secondly, we will use a Multilayer Perceptron in Keras on a _subset_ of the data.  In both cases, we will evaluate the performance using [Matthews correlation coefficient (MCC)](https://en.wikipedia.org/wiki/Matthews_correlation_coefficient). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom evaluation metric in LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM does not include MCC as a metric, but fortunately LightGBM makes it very easy to implement it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import matthews_corrcoef as mcc\n",
    "def mcc_error(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    return 'mcc_error', mcc(preds.round(), labels), True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last line of `mcc_error`, it is very important to use `pred.round()` instead of `preds`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the data into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=['Class','Time'])\n",
    "y = data['Class']\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=1)\n",
    "X_train, X_val, y_train, y_val= train_test_split(X_train, y_train, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_data = lgb.Dataset(X_train, label=y_train)\n",
    "lgb_val = lgb.Dataset(X_val, label=y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lgb hyperparameters below were obtained with [Bayesian Optimization](https://github.com/fmfn/BayesianOptimization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg = lgb.LGBMClassifier(task='train',\n",
    "    boosting_type='gbdt',\n",
    "    objective='binary',\n",
    "    feval = 'mcc_error',\n",
    "    num_leaves= 20,\n",
    "    learning_rate= 0.05,\n",
    "    feature_fraction= 0.8899,\n",
    "    bagging_fraction= 0.8688,\n",
    "    bagging_freq= 20,\n",
    "    scale_pos_weight = 0.00173,\n",
    "    verbose=0,\n",
    "    min_data_in_leaf=3,\n",
    "    n_estimators=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00     45492\n",
      "          1       1.00      0.78      0.88        77\n",
      "\n",
      "avg / total       1.00      1.00      1.00     45569\n",
      "\n",
      "0.8825699402104176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/craigkleski/Library/Python/3.6/lib/python/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "lg.fit(X_train, y_train)\n",
    "pred_val = lg.predict(X_val)\n",
    "print(classification_report(y_val, pred_val))\n",
    "print(mcc(y_val, pred_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a solid score. Let's see how it performs on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8463345814534691\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/craigkleski/Library/Python/3.6/lib/python/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "pred_test = lg.predict(X_test)\n",
    "score = mcc(y_test, pred_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our second approach, we construct an MLP (i.e. all dense layers) in Keras on a subset of the data.  Since we are not using `Time` as one of the features, the order of the transactions is irrelevant.  Because of this, we'll first sort the data set by `Class`, then pick out a set of somewhat balanced transactions. The idea is borrowed from [this](https://www.kaggle.com/randyrose2017/using-scikit-learn-and-keras-for-fraud-detection/notebook) notebook. Yet unlike that notebook, we will remove `Amount` --- the feature with very high VIF --- for a performance boost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sorted = data.sort_values(by='Class', ascending=False, inplace=False)\n",
    "data_sorted = data_sorted.drop(columns=['Time','Amount'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9508\n",
       "1     492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sample = data_sorted.iloc[:10000,:]\n",
    "df_sample.Class.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we divide into training, validation, and test sets, being careful to shuffle the data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "shuffle_df = shuffle(df_sample, random_state=42)\n",
    "\n",
    "df_train = shuffle_df[0:6400]\n",
    "df_val = shuffle_df[6400:8000]\n",
    "df_test = shuffle_df[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_feature = np.array(df_train.values[:,0:28])\n",
    "train_label = np.array(df_train.values[:,-1])\n",
    "val_feature = np.array(df_val.values[:,0:28])\n",
    "val_label = np.array(df_val.values[:,-1])\n",
    "test_feature = np.array(df_test.values[:,0:28])\n",
    "test_label = np.array(df_test.values[:,-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the labels to categorical and scaling the features are both standard techniques in deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(train_label, 2)\n",
    "Y_test = np_utils.to_categorical(test_label, 2)\n",
    "Y_val = np_utils.to_categorical(val_label, 2)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(train_feature)\n",
    "X_tr = scaler.transform(train_feature)\n",
    "X_va = scaler.transform(val_feature)\n",
    "X_te = scaler.transform(test_feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras no longer seems to have the Matthews correlation coefficient as an evaluation metric, but you can still find the code on github to make your own.  Here it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "def matthews_correlation(y_true, y_pred):\n",
    "    ''' Matthews correlation coefficient\n",
    "    '''\n",
    "    y_pred_pos = K.round(K.clip(y_pred, 0, 1))\n",
    "    y_pred_neg = 1 - y_pred_pos\n",
    "\n",
    "    y_pos = K.round(K.clip(y_true, 0, 1))\n",
    "    y_neg = 1 - y_pos\n",
    "\n",
    "    tp = K.sum(y_pos * y_pred_pos)\n",
    "    tn = K.sum(y_neg * y_pred_neg)\n",
    "    \n",
    "    fp = K.sum(y_neg * y_pred_pos)\n",
    "    fn = K.sum(y_pos * y_pred_neg)\n",
    "\n",
    "    \n",
    "    numerator = (tp * tn - fp * fn)\n",
    "    denominator = K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))\n",
    "\n",
    "    return numerator / (denominator + K.epsilon())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is our MLP implemented in Keras.  We use the `TruncatedNormal` initializer in our layers to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 256)               7424      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 139,522\n",
      "Trainable params: 139,522\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 6400 samples, validate on 1600 samples\n",
      "Epoch 1/100\n",
      "6400/6400 [==============================] - 1s 178us/step - loss: 0.3620 - matthews_correlation: 0.8367 - val_loss: 0.1678 - val_matthews_correlation: 0.9125\n",
      "Epoch 2/100\n",
      "6400/6400 [==============================] - ETA: 0s - loss: 0.1877 - matthews_correlation: 0.89 - 0s 74us/step - loss: 0.1894 - matthews_correlation: 0.8963 - val_loss: 0.1458 - val_matthews_correlation: 0.9125\n",
      "Epoch 3/100\n",
      "6400/6400 [==============================] - 0s 66us/step - loss: 0.1626 - matthews_correlation: 0.8963 - val_loss: 0.1168 - val_matthews_correlation: 0.9125\n",
      "Epoch 4/100\n",
      "6400/6400 [==============================] - 0s 68us/step - loss: 0.1241 - matthews_correlation: 0.8970 - val_loss: 0.0923 - val_matthews_correlation: 0.9125\n",
      "Epoch 5/100\n",
      "6400/6400 [==============================] - 0s 68us/step - loss: 0.0962 - matthews_correlation: 0.9147 - val_loss: 0.0768 - val_matthews_correlation: 0.9606\n",
      "Epoch 6/100\n",
      "6400/6400 [==============================] - 0s 61us/step - loss: 0.0792 - matthews_correlation: 0.9478 - val_loss: 0.0684 - val_matthews_correlation: 0.9838\n",
      "Epoch 7/100\n",
      "6400/6400 [==============================] - 1s 81us/step - loss: 0.0738 - matthews_correlation: 0.9667 - val_loss: 0.0509 - val_matthews_correlation: 0.9850\n",
      "Epoch 8/100\n",
      "6400/6400 [==============================] - 0s 65us/step - loss: 0.0591 - matthews_correlation: 0.9773 - val_loss: 0.0443 - val_matthews_correlation: 0.9806\n",
      "Epoch 9/100\n",
      "6400/6400 [==============================] - 0s 70us/step - loss: 0.0498 - matthews_correlation: 0.9759 - val_loss: 0.0412 - val_matthews_correlation: 0.9825\n",
      "Epoch 10/100\n",
      "6400/6400 [==============================] - 0s 66us/step - loss: 0.0509 - matthews_correlation: 0.9772 - val_loss: 0.0408 - val_matthews_correlation: 0.9813\n",
      "Epoch 11/100\n",
      "6400/6400 [==============================] - 0s 67us/step - loss: 0.0482 - matthews_correlation: 0.9772 - val_loss: 0.0384 - val_matthews_correlation: 0.9850\n",
      "Epoch 12/100\n",
      "6400/6400 [==============================] - 0s 67us/step - loss: 0.0492 - matthews_correlation: 0.9777 - val_loss: 0.0435 - val_matthews_correlation: 0.9800\n",
      "Epoch 13/100\n",
      "6400/6400 [==============================] - 0s 63us/step - loss: 0.0495 - matthews_correlation: 0.9775 - val_loss: 0.0401 - val_matthews_correlation: 0.9825\n",
      "Epoch 14/100\n",
      "6400/6400 [==============================] - 0s 68us/step - loss: 0.0446 - matthews_correlation: 0.9798 - val_loss: 0.0382 - val_matthews_correlation: 0.9850\n",
      "Epoch 15/100\n",
      "6400/6400 [==============================] - 0s 63us/step - loss: 0.0490 - matthews_correlation: 0.9780 - val_loss: 0.0377 - val_matthews_correlation: 0.9850\n",
      "Epoch 16/100\n",
      "6400/6400 [==============================] - 0s 63us/step - loss: 0.0513 - matthews_correlation: 0.9766 - val_loss: 0.0374 - val_matthews_correlation: 0.9850\n",
      "Epoch 17/100\n",
      "6400/6400 [==============================] - 0s 68us/step - loss: 0.0483 - matthews_correlation: 0.9788 - val_loss: 0.0455 - val_matthews_correlation: 0.9813\n",
      "Epoch 18/100\n",
      "6400/6400 [==============================] - 0s 68us/step - loss: 0.0460 - matthews_correlation: 0.9792 - val_loss: 0.0377 - val_matthews_correlation: 0.9850\n",
      "Epoch 19/100\n",
      "6400/6400 [==============================] - 0s 65us/step - loss: 0.0414 - matthews_correlation: 0.9795 - val_loss: 0.0377 - val_matthews_correlation: 0.9838\n",
      "Epoch 20/100\n",
      "6400/6400 [==============================] - 0s 61us/step - loss: 0.0442 - matthews_correlation: 0.9797 - val_loss: 0.0416 - val_matthews_correlation: 0.9813\n",
      "Epoch 21/100\n",
      "6400/6400 [==============================] - 0s 68us/step - loss: 0.0447 - matthews_correlation: 0.9800 - val_loss: 0.0422 - val_matthews_correlation: 0.9763\n",
      "Epoch 22/100\n",
      "6400/6400 [==============================] - 0s 65us/step - loss: 0.0446 - matthews_correlation: 0.9802 - val_loss: 0.0397 - val_matthews_correlation: 0.9825\n",
      "Epoch 23/100\n",
      "6400/6400 [==============================] - 0s 69us/step - loss: 0.0418 - matthews_correlation: 0.9803 - val_loss: 0.0376 - val_matthews_correlation: 0.9838\n",
      "Epoch 24/100\n",
      "6400/6400 [==============================] - 0s 64us/step - loss: 0.0462 - matthews_correlation: 0.9783 - val_loss: 0.0367 - val_matthews_correlation: 0.9863\n",
      "Epoch 25/100\n",
      "6400/6400 [==============================] - 0s 58us/step - loss: 0.0430 - matthews_correlation: 0.9811 - val_loss: 0.0476 - val_matthews_correlation: 0.9788\n",
      "Epoch 26/100\n",
      "6400/6400 [==============================] - 0s 68us/step - loss: 0.0436 - matthews_correlation: 0.9792 - val_loss: 0.0378 - val_matthews_correlation: 0.9850\n",
      "Epoch 27/100\n",
      "6400/6400 [==============================] - 0s 66us/step - loss: 0.0454 - matthews_correlation: 0.9788 - val_loss: 0.0369 - val_matthews_correlation: 0.9850\n",
      "Epoch 28/100\n",
      "6400/6400 [==============================] - 0s 69us/step - loss: 0.0447 - matthews_correlation: 0.9791 - val_loss: 0.0511 - val_matthews_correlation: 0.9750\n",
      "Epoch 29/100\n",
      "6400/6400 [==============================] - 0s 68us/step - loss: 0.0475 - matthews_correlation: 0.9789 - val_loss: 0.0477 - val_matthews_correlation: 0.9800\n",
      "Epoch 30/100\n",
      "6400/6400 [==============================] - 0s 70us/step - loss: 0.0445 - matthews_correlation: 0.9809 - val_loss: 0.0401 - val_matthews_correlation: 0.9756\n",
      "Epoch 31/100\n",
      "6400/6400 [==============================] - 1s 90us/step - loss: 0.0474 - matthews_correlation: 0.9773 - val_loss: 0.0463 - val_matthews_correlation: 0.9813\n",
      "Epoch 32/100\n",
      "6400/6400 [==============================] - 0s 69us/step - loss: 0.0472 - matthews_correlation: 0.9792 - val_loss: 0.0366 - val_matthews_correlation: 0.9850\n",
      "Epoch 33/100\n",
      "6400/6400 [==============================] - 0s 70us/step - loss: 0.0490 - matthews_correlation: 0.9803 - val_loss: 0.0363 - val_matthews_correlation: 0.9863\n",
      "Epoch 34/100\n",
      "6400/6400 [==============================] - 0s 68us/step - loss: 0.0414 - matthews_correlation: 0.9800 - val_loss: 0.0368 - val_matthews_correlation: 0.9863\n",
      "Epoch 35/100\n",
      "6400/6400 [==============================] - 0s 72us/step - loss: 0.0450 - matthews_correlation: 0.9802 - val_loss: 0.0379 - val_matthews_correlation: 0.9850\n",
      "Epoch 36/100\n",
      "6400/6400 [==============================] - 0s 73us/step - loss: 0.0432 - matthews_correlation: 0.9808 - val_loss: 0.0370 - val_matthews_correlation: 0.9850\n",
      "Epoch 37/100\n",
      "6400/6400 [==============================] - 0s 71us/step - loss: 0.0439 - matthews_correlation: 0.9797 - val_loss: 0.0369 - val_matthews_correlation: 0.9850\n",
      "Epoch 38/100\n",
      "6400/6400 [==============================] - 0s 64us/step - loss: 0.0441 - matthews_correlation: 0.9808 - val_loss: 0.0380 - val_matthews_correlation: 0.9800\n",
      "Epoch 39/100\n",
      "6400/6400 [==============================] - 1s 79us/step - loss: 0.0416 - matthews_correlation: 0.9808 - val_loss: 0.0368 - val_matthews_correlation: 0.9850\n",
      "Epoch 40/100\n",
      "6400/6400 [==============================] - 0s 68us/step - loss: 0.0445 - matthews_correlation: 0.9806 - val_loss: 0.0373 - val_matthews_correlation: 0.9838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100\n",
      "6400/6400 [==============================] - 0s 71us/step - loss: 0.0421 - matthews_correlation: 0.9806 - val_loss: 0.0382 - val_matthews_correlation: 0.9800\n",
      "Epoch 42/100\n",
      "6400/6400 [==============================] - 0s 66us/step - loss: 0.0420 - matthews_correlation: 0.9806 - val_loss: 0.0371 - val_matthews_correlation: 0.9863\n",
      "Epoch 43/100\n",
      "6400/6400 [==============================] - 0s 75us/step - loss: 0.0433 - matthews_correlation: 0.9789 - val_loss: 0.0373 - val_matthews_correlation: 0.9825\n",
      "Epoch 44/100\n",
      "6400/6400 [==============================] - 0s 73us/step - loss: 0.0394 - matthews_correlation: 0.9825 - val_loss: 0.0373 - val_matthews_correlation: 0.9838\n",
      "Epoch 45/100\n",
      "6400/6400 [==============================] - 0s 71us/step - loss: 0.0410 - matthews_correlation: 0.9806 - val_loss: 0.0365 - val_matthews_correlation: 0.9850\n",
      "Epoch 46/100\n",
      "6400/6400 [==============================] - 0s 69us/step - loss: 0.0417 - matthews_correlation: 0.9825 - val_loss: 0.0366 - val_matthews_correlation: 0.9838\n",
      "Epoch 47/100\n",
      "6400/6400 [==============================] - 0s 65us/step - loss: 0.0413 - matthews_correlation: 0.9816 - val_loss: 0.0398 - val_matthews_correlation: 0.9813\n",
      "Epoch 48/100\n",
      "6400/6400 [==============================] - 0s 69us/step - loss: 0.0437 - matthews_correlation: 0.9791 - val_loss: 0.0424 - val_matthews_correlation: 0.9813\n",
      "Epoch 49/100\n",
      "6400/6400 [==============================] - 0s 68us/step - loss: 0.0421 - matthews_correlation: 0.9809 - val_loss: 0.0371 - val_matthews_correlation: 0.9850\n",
      "Epoch 50/100\n",
      "6400/6400 [==============================] - 0s 70us/step - loss: 0.0440 - matthews_correlation: 0.9803 - val_loss: 0.0365 - val_matthews_correlation: 0.9850\n",
      "Epoch 51/100\n",
      "6400/6400 [==============================] - 0s 66us/step - loss: 0.0430 - matthews_correlation: 0.9803 - val_loss: 0.0374 - val_matthews_correlation: 0.9838\n",
      "Epoch 52/100\n",
      "6400/6400 [==============================] - 0s 69us/step - loss: 0.0456 - matthews_correlation: 0.9808 - val_loss: 0.0367 - val_matthews_correlation: 0.9856\n",
      "Epoch 53/100\n",
      "6400/6400 [==============================] - 0s 67us/step - loss: 0.0401 - matthews_correlation: 0.9809 - val_loss: 0.0372 - val_matthews_correlation: 0.9850\n",
      "Epoch 54/100\n",
      "6400/6400 [==============================] - 0s 71us/step - loss: 0.0411 - matthews_correlation: 0.9811 - val_loss: 0.0384 - val_matthews_correlation: 0.9825\n",
      "Epoch 55/100\n",
      "6400/6400 [==============================] - 0s 71us/step - loss: 0.0404 - matthews_correlation: 0.9816 - val_loss: 0.0386 - val_matthews_correlation: 0.9788\n",
      "Epoch 56/100\n",
      "6400/6400 [==============================] - 0s 63us/step - loss: 0.0408 - matthews_correlation: 0.9814 - val_loss: 0.0360 - val_matthews_correlation: 0.9863\n",
      "Epoch 57/100\n",
      "6400/6400 [==============================] - 0s 70us/step - loss: 0.0446 - matthews_correlation: 0.9786 - val_loss: 0.0389 - val_matthews_correlation: 0.9813\n",
      "Epoch 58/100\n",
      "6400/6400 [==============================] - 0s 73us/step - loss: 0.0412 - matthews_correlation: 0.9811 - val_loss: 0.0375 - val_matthews_correlation: 0.9794\n",
      "Epoch 59/100\n",
      "6400/6400 [==============================] - 0s 69us/step - loss: 0.0424 - matthews_correlation: 0.9803 - val_loss: 0.0415 - val_matthews_correlation: 0.9813\n",
      "Epoch 60/100\n",
      "6400/6400 [==============================] - 0s 69us/step - loss: 0.0429 - matthews_correlation: 0.9825 - val_loss: 0.0388 - val_matthews_correlation: 0.9850\n",
      "Epoch 61/100\n",
      "6400/6400 [==============================] - 0s 71us/step - loss: 0.0413 - matthews_correlation: 0.9817 - val_loss: 0.0364 - val_matthews_correlation: 0.9863\n",
      "Epoch 62/100\n",
      "6400/6400 [==============================] - 0s 71us/step - loss: 0.0401 - matthews_correlation: 0.9800 - val_loss: 0.0402 - val_matthews_correlation: 0.9863\n",
      "Epoch 63/100\n",
      "6400/6400 [==============================] - 0s 78us/step - loss: 0.0425 - matthews_correlation: 0.9814 - val_loss: 0.0368 - val_matthews_correlation: 0.9850\n",
      "Epoch 64/100\n",
      "6400/6400 [==============================] - 0s 73us/step - loss: 0.0413 - matthews_correlation: 0.9819 - val_loss: 0.0367 - val_matthews_correlation: 0.9863\n",
      "Epoch 65/100\n",
      "6400/6400 [==============================] - 0s 68us/step - loss: 0.0398 - matthews_correlation: 0.9820 - val_loss: 0.0363 - val_matthews_correlation: 0.9863\n",
      "Epoch 66/100\n",
      "6400/6400 [==============================] - 0s 70us/step - loss: 0.0446 - matthews_correlation: 0.9805 - val_loss: 0.0359 - val_matthews_correlation: 0.9850\n",
      "Epoch 67/100\n",
      "6400/6400 [==============================] - 0s 70us/step - loss: 0.0421 - matthews_correlation: 0.9820 - val_loss: 0.0368 - val_matthews_correlation: 0.9850\n",
      "Epoch 68/100\n",
      "6400/6400 [==============================] - 0s 71us/step - loss: 0.0417 - matthews_correlation: 0.9816 - val_loss: 0.0361 - val_matthews_correlation: 0.9850\n",
      "Epoch 69/100\n",
      "6400/6400 [==============================] - 0s 65us/step - loss: 0.0407 - matthews_correlation: 0.9813 - val_loss: 0.0362 - val_matthews_correlation: 0.9863\n",
      "Epoch 70/100\n",
      "6400/6400 [==============================] - 0s 68us/step - loss: 0.0414 - matthews_correlation: 0.9808 - val_loss: 0.0378 - val_matthews_correlation: 0.9825\n",
      "Epoch 71/100\n",
      "6400/6400 [==============================] - 0s 67us/step - loss: 0.0411 - matthews_correlation: 0.9814 - val_loss: 0.0365 - val_matthews_correlation: 0.9838\n",
      "Epoch 72/100\n",
      "6400/6400 [==============================] - 0s 68us/step - loss: 0.0402 - matthews_correlation: 0.9805 - val_loss: 0.0363 - val_matthews_correlation: 0.9863\n",
      "Epoch 73/100\n",
      "6400/6400 [==============================] - 0s 68us/step - loss: 0.0400 - matthews_correlation: 0.9817 - val_loss: 0.0408 - val_matthews_correlation: 0.9750\n",
      "Epoch 74/100\n",
      "6400/6400 [==============================] - 0s 64us/step - loss: 0.0419 - matthews_correlation: 0.9817 - val_loss: 0.0389 - val_matthews_correlation: 0.9813\n",
      "Epoch 75/100\n",
      "6400/6400 [==============================] - 0s 70us/step - loss: 0.0439 - matthews_correlation: 0.9814 - val_loss: 0.0412 - val_matthews_correlation: 0.9763\n",
      "Epoch 76/100\n",
      "6400/6400 [==============================] - 0s 68us/step - loss: 0.0458 - matthews_correlation: 0.9797 - val_loss: 0.0377 - val_matthews_correlation: 0.9838\n",
      "Epoch 77/100\n",
      "6400/6400 [==============================] - 0s 67us/step - loss: 0.0406 - matthews_correlation: 0.9814 - val_loss: 0.0372 - val_matthews_correlation: 0.9838\n",
      "Epoch 78/100\n",
      "6400/6400 [==============================] - 0s 66us/step - loss: 0.0436 - matthews_correlation: 0.9781 - val_loss: 0.0366 - val_matthews_correlation: 0.9838\n",
      "Epoch 79/100\n",
      "6400/6400 [==============================] - 0s 67us/step - loss: 0.0394 - matthews_correlation: 0.9806 - val_loss: 0.0430 - val_matthews_correlation: 0.9738\n",
      "Epoch 80/100\n",
      "6400/6400 [==============================] - 0s 72us/step - loss: 0.0435 - matthews_correlation: 0.9816 - val_loss: 0.0454 - val_matthews_correlation: 0.9788\n",
      "Epoch 81/100\n",
      "6400/6400 [==============================] - 0s 72us/step - loss: 0.0445 - matthews_correlation: 0.9795 - val_loss: 0.0361 - val_matthews_correlation: 0.9850\n",
      "Epoch 82/100\n",
      "6400/6400 [==============================] - 0s 72us/step - loss: 0.0421 - matthews_correlation: 0.9803 - val_loss: 0.0371 - val_matthews_correlation: 0.9825\n",
      "Epoch 83/100\n",
      "6400/6400 [==============================] - 0s 66us/step - loss: 0.0419 - matthews_correlation: 0.9803 - val_loss: 0.0394 - val_matthews_correlation: 0.9813\n",
      "Epoch 84/100\n",
      "6400/6400 [==============================] - 0s 69us/step - loss: 0.0388 - matthews_correlation: 0.9817 - val_loss: 0.0379 - val_matthews_correlation: 0.9850\n",
      "Epoch 85/100\n",
      "6400/6400 [==============================] - 0s 75us/step - loss: 0.0440 - matthews_correlation: 0.9803 - val_loss: 0.0396 - val_matthews_correlation: 0.9775\n",
      "Epoch 86/100\n",
      "6400/6400 [==============================] - 0s 76us/step - loss: 0.0413 - matthews_correlation: 0.9800 - val_loss: 0.0401 - val_matthews_correlation: 0.9738\n",
      "Epoch 87/100\n",
      "6400/6400 [==============================] - 0s 62us/step - loss: 0.0374 - matthews_correlation: 0.9806 - val_loss: 0.0371 - val_matthews_correlation: 0.9850\n",
      "Epoch 88/100\n",
      "6400/6400 [==============================] - 0s 75us/step - loss: 0.0387 - matthews_correlation: 0.9811 - val_loss: 0.0375 - val_matthews_correlation: 0.9806\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/100\n",
      "6400/6400 [==============================] - 0s 74us/step - loss: 0.0409 - matthews_correlation: 0.9808 - val_loss: 0.0370 - val_matthews_correlation: 0.9850\n",
      "Epoch 90/100\n",
      "6400/6400 [==============================] - 0s 74us/step - loss: 0.0411 - matthews_correlation: 0.9838 - val_loss: 0.0368 - val_matthews_correlation: 0.9850\n",
      "Epoch 91/100\n",
      "6400/6400 [==============================] - 0s 70us/step - loss: 0.0409 - matthews_correlation: 0.9813 - val_loss: 0.0366 - val_matthews_correlation: 0.9856\n",
      "Epoch 92/100\n",
      "6400/6400 [==============================] - 0s 73us/step - loss: 0.0425 - matthews_correlation: 0.9797 - val_loss: 0.0368 - val_matthews_correlation: 0.9863\n",
      "Epoch 93/100\n",
      "6400/6400 [==============================] - 0s 69us/step - loss: 0.0397 - matthews_correlation: 0.9809 - val_loss: 0.0363 - val_matthews_correlation: 0.9863\n",
      "Epoch 94/100\n",
      "6400/6400 [==============================] - 0s 74us/step - loss: 0.0421 - matthews_correlation: 0.9813 - val_loss: 0.0366 - val_matthews_correlation: 0.9863\n",
      "Epoch 95/100\n",
      "6400/6400 [==============================] - 0s 69us/step - loss: 0.0418 - matthews_correlation: 0.9806 - val_loss: 0.0376 - val_matthews_correlation: 0.9850\n",
      "Epoch 96/100\n",
      "6400/6400 [==============================] - 0s 71us/step - loss: 0.0412 - matthews_correlation: 0.9814 - val_loss: 0.0365 - val_matthews_correlation: 0.9863\n",
      "Epoch 97/100\n",
      "6400/6400 [==============================] - 0s 70us/step - loss: 0.0386 - matthews_correlation: 0.9816 - val_loss: 0.0371 - val_matthews_correlation: 0.9838\n",
      "Epoch 98/100\n",
      "6400/6400 [==============================] - 0s 68us/step - loss: 0.0433 - matthews_correlation: 0.9783 - val_loss: 0.0370 - val_matthews_correlation: 0.9850\n",
      "Epoch 99/100\n",
      "6400/6400 [==============================] - 0s 71us/step - loss: 0.0427 - matthews_correlation: 0.9794 - val_loss: 0.0366 - val_matthews_correlation: 0.9850\n",
      "Epoch 100/100\n",
      "6400/6400 [==============================] - 0s 66us/step - loss: 0.0400 - matthews_correlation: 0.9822 - val_loss: 0.0377 - val_matthews_correlation: 0.9863\n",
      "[0.043402254733257, 0.983]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential() \n",
    "model.add(Dense(units=256, \n",
    "                input_dim=28, \n",
    "                kernel_initializer='TruncatedNormal', \n",
    "                activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=256,  \n",
    "                kernel_initializer='TruncatedNormal', \n",
    "                activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=256,  \n",
    "                kernel_initializer='TruncatedNormal', \n",
    "                activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units=2,\n",
    "                kernel_initializer='TruncatedNormal',\n",
    "                activation='sigmoid'))\n",
    "\n",
    "print(model.summary()) \n",
    "\n",
    "model.compile(loss='binary_crossentropy',  \n",
    "              optimizer='adam', metrics=[matthews_correlation])\n",
    "\n",
    "model.fit(x=X_tr, y=Y_train,  \n",
    "                          validation_data=(X_va, Y_val), epochs=100, \n",
    "                          batch_size=300, verbose=1) \n",
    "\n",
    "\n",
    "score = model.evaluate(X_te, Y_test, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_te = model.predict_classes(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        0.0       0.99      1.00      1.00      1910\n",
      "        1.0       0.96      0.84      0.90        90\n",
      "\n",
      "avg / total       0.99      0.99      0.99      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_label, pred_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8970636232683191\n"
     ]
    }
   ],
   "source": [
    "score = mcc(test_label, pred_te)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
