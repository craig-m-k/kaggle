{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognizer: Vision Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Version 1: 05/01/18_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an analysis of the MNIST digit data set using a Convolution Neural Network (CNN).  The CNN is implemented with Keras on top of Tensorflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set can be found on [kaggle](https://www.kaggle.com/c/digit-recognizer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/craigkleski/Library/Python/3.6/lib/python/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Convolution2D\n",
    "from keras.layers import MaxPooling2D, Flatten\n",
    "from keras.utils import np_utils\n",
    "\n",
    "np.random.seed(149)\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and format the data for use in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[~train['label'].isnull()].drop(columns=['label'])\n",
    "y = train['label']\n",
    "\n",
    "# Use only underlying numpy array\n",
    "X = X.values\n",
    "\n",
    "# each image is 28x28. to use CNN, the data must be reshaped\n",
    "X = X.reshape(-1,28, 28, 1)\n",
    "\n",
    "# rescale the pixel values to be in the interval [0,1]\n",
    "X = X.astype(float)\n",
    "X /= 255\n",
    "\n",
    "# create train-test split. X_te and y_te are our validation set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.1)\n",
    "\n",
    "# one-hot encode the 10 digit categories\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the shape of X_tr and y_tr to make sure it looks OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37800, 28, 28, 1) (37800,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the CNN. The Keras `Sequential` model stacks linear layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "37800/37800 [==============================] - 163s 4ms/step - loss: 0.2366 - acc: 0.9251\n",
      "Epoch 2/10\n",
      "37800/37800 [==============================] - 159s 4ms/step - loss: 0.0862 - acc: 0.9733\n",
      "Epoch 3/10\n",
      "37800/37800 [==============================] - 160s 4ms/step - loss: 0.0649 - acc: 0.9798\n",
      "Epoch 4/10\n",
      "37800/37800 [==============================] - 166s 4ms/step - loss: 0.0573 - acc: 0.9830\n",
      "Epoch 5/10\n",
      "37800/37800 [==============================] - 178s 5ms/step - loss: 0.0483 - acc: 0.9854\n",
      "Epoch 6/10\n",
      "37800/37800 [==============================] - 184s 5ms/step - loss: 0.0453 - acc: 0.9868\n",
      "Epoch 7/10\n",
      "37800/37800 [==============================] - 187s 5ms/step - loss: 0.0415 - acc: 0.9866\n",
      "Epoch 8/10\n",
      "37800/37800 [==============================] - 189s 5ms/step - loss: 0.0359 - acc: 0.9894\n",
      "Epoch 9/10\n",
      "37800/37800 [==============================] - 191s 5ms/step - loss: 0.0346 - acc: 0.9894\n",
      "Epoch 10/10\n",
      "37800/37800 [==============================] - 192s 5ms/step - loss: 0.0316 - acc: 0.9896\n",
      "[0.02959665256060405, 0.9904761904761905]\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# for the initial convolution we specify the input shape\n",
    "model.add(Convolution2D(32, (5, 5), activation='relu', input_shape=(28,28,1)))\n",
    "\n",
    "# add convolution and pooling layers. dropout to prevent overfitting\n",
    "model.add(Convolution2D(32, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Convolution2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "# flattens the input array\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# final layer for 10 prediction classes\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    " \n",
    "# fit model on training set\n",
    "model.fit(X_train, Y_train, batch_size=32, epochs=10, verbose=1)\n",
    "\n",
    "# see how we did\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I experimented with [BatchNormalization](https://keras.io/layers/normalization/) but it didn't improve my score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now reshape the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_p = test.values\n",
    "X_p = X_p.astype(float)\n",
    "X_p /= 255\n",
    "X_p = X_p.reshape(-1,28, 28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_classes(X_p)\n",
    "res = pd.DataFrame()\n",
    "res['ImageId'] = list(test.index+1)\n",
    "res['Label'] = pred\n",
    "res.to_csv('digits.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
